{
  "$id": "$gai-technique/evade_ml_model",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "Adversaries can craft adversarial data that prevent a machine learning model from correctly identifying the contents of the data. This technique can be used to evade a downstream task where machine learning is utilized. The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.",
  "external_references": [],
  "framework_references": [
    {
      "framework_id": "AML.T0015",
      "framework_name": "MITRE ATLAS",
      "href": "https://atlas.mitre.org/techniques/AML.T0015"
    }
  ],
  "name": "Evade ML Model",
  "object_references": [
    {
      "$id": "$gai-tactic/initial_access",
      "$type": "tactic",
      "description": "Bypassing or evading machine learning models used for security or detection to gain unauthorized access."
    },
    {
      "$id": "$gai-tactic/defense_evasion",
      "$type": "tactic",
      "description": "Evading detection or mitigation measures implemented by machine learning models."
    },
    {
      "$id": "$gai-tactic/impact",
      "$type": "tactic",
      "description": "Manipulating machine learning models to evade detection can lead to severe security breaches."
    }
  ]
}
