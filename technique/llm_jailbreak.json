{
  "$id": "$gai-technique/llm_jailbreak",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "An adversary may use a carefully crafted prompt injection designed to place LLM in a state in which it will freely respond to any user input, bypassing any controls, restrictions, or guardrails placed on the LLM. Once successfully jailbroken, the LLM can be used in unintended ways by the adversary..",
  "external_references": [
    {
      "href": "https://github.com/elder-plinius/L1B3RT45",
      "source": "Github",
      "title": "L1B3RT45: Jailbreak prompts for all major AI models."
    },
    {
      "href": "https://simonwillison.net/2024/Mar/5/prompt-injection-jailbreaking/",
      "source": "Simon Willison's blog",
      "title": "Prompt injection and jailbreaking are not the same thing."
    },
    {
      "href": "https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/",
      "source": "Microsoft",
      "title": "AI jailbreaks: What they are and how they can be mitigated."
    }
  ],
  "framework_references": [
    {
      "framework_id": "AML.T0054",
      "framework_name": "MITRE ATLAS",
      "href": "https://atlas.mitre.org/techniques/AML.T0054"
    }
  ],
  "name": "LLM Jailbreak",
  "object_references": [
    {
      "$id": "$gai-tactic/privilege_escalation",
      "$type": "tactic",
      "description": "An adversary can override system-level prompts using user-level prompts."
    },
    {
      "$id": "$gai-tactic/defense_evasion",
      "$type": "tactic",
      "description": "An adversary can bypass detections by jailbreaking the LLM."
    }
  ]
}
